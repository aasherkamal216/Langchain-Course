{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Webpage content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.1/docs/expression_language/streaming/\")\n",
    "webpage_content = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://python.langchain.com/v0.1/docs/expression_language/streaming/'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Streaming | ü¶úÔ∏èüîó LangChain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Streaming is critical in making applications based on LLMs feel responsive to end-users.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'is a declarative way to specify a \"program\" by chainining together different LangChain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">primitives. Chains created using LCEL benefit from an automatic implementation of stream and astream allowing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">StrOutputParser()chain = prompt | model | parserasync for chunk in chain.astream({\"topic\": \"parrot\"}):    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">print(chunk, end=\"|\", flush=True)API Reference:StrOutputParserChatPromptTemplate Here|\\'s| a| silly| joke| about| </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a| par|rot|:|What| kind| of| teacher| gives| good| advice|?| An| ap|-|parent| (|app|arent|)| one|!||You might </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">notice above that parser actually doesn\\'t block the streaming output from the model, and instead processes each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">chunk individually. Many of the LCEL primitives also'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'source'\u001b[0m: \u001b[32m'https://python.langchain.com/v0.1/docs/expression_language/streaming/'\u001b[0m,\n",
       "        \u001b[32m'title'\u001b[0m: \u001b[32m'Streaming | ü¶úÔ∏èüîó LangChain'\u001b[0m,\n",
       "        \u001b[32m'description'\u001b[0m: \u001b[32m'Streaming is critical in making applications based on LLMs feel responsive to end-users.'\u001b[0m,\n",
       "        \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'is a declarative way to specify a \"program\" by chainining together different LangChain \u001b[0m\n",
       "\u001b[32mprimitives. Chains created using LCEL benefit from an automatic implementation of stream and astream allowing \u001b[0m\n",
       "\u001b[32mstreaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable \u001b[0m\n",
       "\u001b[32minterface.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import \u001b[0m\n",
       "\u001b[32mChatPromptTemplateprompt = ChatPromptTemplate.from_template\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32mparser = \u001b[0m\n",
       "\u001b[32mStrOutputParser\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mchain = prompt | model | parserasync for chunk in chain.astream\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"topic\": \"parrot\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:    \u001b[0m\n",
       "\u001b[32mprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mchunk, \u001b[0m\u001b[32mend\u001b[0m\u001b[32m=\"|\", \u001b[0m\u001b[32mflush\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m)\u001b[0m\u001b[32mAPI Reference:StrOutputParserChatPromptTemplate Here|\\'s| a| silly| joke| about| \u001b[0m\n",
       "\u001b[32ma| par|rot|:|What| kind| of| teacher| gives| good| advice|?| An| ap|-|parent| \u001b[0m\u001b[32m(\u001b[0m\u001b[32m|app|arent|\u001b[0m\u001b[32m)\u001b[0m\u001b[32m| one|!||You might \u001b[0m\n",
       "\u001b[32mnotice above that parser actually doesn\\'t block the streaming output from the model, and instead processes each \u001b[0m\n",
       "\u001b[32mchunk individually. Many of the LCEL primitives also'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(webpage_content)\n",
    "print(chunks[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing chunks in vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002567269C4D0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "db = Chroma.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are a helpful assistant for Q/A tasks. Use the following pieces of\\n    retrieved content to answer the question. If you don't know the answer, just say\\n    that you don't know. Keep the answer concise.\\n\\n\\n    Context:\\n    {context}\\n    \")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a helpful assistant for Q/A tasks. Use the following pieces of\n",
    "    retrieved content to answer the question. If you don't know the answer, just say\n",
    "    that you don't know. Keep the answer concise.\\n\\n\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002567269C4D0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are a helpful assistant for Q/A tasks. Use the following pieces of\\n    retrieved content to answer the question. If you don't know the answer, just say\\n    that you don't know. Keep the answer concise.\\n\\n\\n    Context:\\n    {context}\\n    \")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000256772A0D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000256768DD110>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Streaming?',\n",
       " 'context': [Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content='Streaming | ü¶úÔ∏èüîó LangChain'),\n",
       "  Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content=\"to end-users.Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.This interface provides two general approaches to stream content:sync stream and async astream: a default implementation of streaming that streams the final output from the chain.async astream_events and async astream_log: these provide a way to stream both intermediate steps and final output from the chain.Let's take a look at both approaches, and try to understand how to use them. ü•∑Using Stream\\u200bAll Runnable objects implement a sync method called stream and an async variant called astream. These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.Streaming is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.The complexity of this processing can vary, from straightforward tasks like\"),\n",
       "  Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content=\"Based| on| the| given| context|,| the| only| information| provided| about| where| Harrison| worked| is| that| he| worked| at| Ken|sh|o|.| Since| there| are| no| other| details| provided| about| Ken|sh|o|,| I| do| not| have| enough| information| to| write| 3| additional| made| up| sentences| about| this| place|.| I| can| only| state| that| Harrison| worked| at| Ken|sh|o|.||Now that we've seen how stream and astream work, let's venture into the world of streaming events. üèûÔ∏èUsing Stream Events\\u200bEvent Streaming is a beta API. This API may change a bit based on feedback.noteIntroduced in langchain-core 0.1.14.import langchain_corelangchain_core.__version__'0.1.18'For the astream_events API to work properly:Use async throughout the code to the extent possible (e.g., async tools etc)Propagate callbacks if defining custom functions / runnablesWhenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.Let us know if\"),\n",
       "  Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content='know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.The best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!LLMs and Chat Models\\u200bLarge language models and their chat variants are the primary bottleneck in LLM based apps. üôäLarge language models can take several seconds to generate a complete response to a query. This is far slower than the ~200-300 ms threshold at which an application feels responsive to an end user.The key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model token by token.We will show examples of streaming using the chat model from Anthropic. To use the')],\n",
       " 'answer': 'Streaming is a way to stream content from LangChain primitives like LLMs, parsers, prompts, retrievers, and agents. It provides two general approaches to stream content: sync stream and async astream. These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Streaming?\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"I couldn't understand it?\",\n",
       " 'context': [Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content='happens if we try to stream them? ü§®from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\", \"harrison likes spicy food\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]chunksAPI Reference:FAISSStrOutputParserChatPromptTemplateRunnablePassthroughOpenAIEmbeddings[[Document(page_content=\\'harrison worked at kensho\\'),  Document(page_content=\\'harrison likes spicy food\\')]]Stream just yielded the final result from that component.This is OK ü•π! Not all components have to implement'),\n",
       "  Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content='Streaming | ü¶úÔ∏èüîó LangChain'),\n",
       "  Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content='and `population`\\',    version=\"v1\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        breakChat model chunk: \\' Here\\'Chat model chunk: \\' is\\'Chat model chunk: \\' the\\'Chat model chunk: \\' JSON\\'Chat model chunk: \\' with\\'Chat model chunk: \\' the\\'Chat model chunk: \\' requested\\'Chat model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat model chunk: \\':\\'Chat model chunk: \\'\\\\n\\\\n```\\'Chat model chunk: \\'json\\'Parser chunk: {}Chat model chunk: \\'\\\\n{\\'Chat model chunk: \\'\\\\n \\'Chat model chunk: \\' \"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: {\\'countries\\': []}Chat model chunk: \\' [\\'Chat model chunk: \\'\\\\n'),\n",
       "  Document(metadata={'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en', 'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain'}, page_content='{\\'chunk\\': AIMessageChunk(content=\\' countries\\')}}...Non-streaming components\\u200bRemember how some components don\\'t stream well because they don\\'t operate on input streams?While such components can break streaming of the final output when using astream, astream_events will still yield streaming events from intermediate steps that support streaming!# Function that does not support streaming.# It operates on the finalizes inputs rather than# operating on the input stream.def _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = (    model | JsonOutputParser() | _extract_country_names)  # This parser only')],\n",
       " 'answer': \"I don't know. This text seems to be about the LangChain library and how to handle streaming data. It appears to be more of a reference or a guide rather than a clear question and answer. If you have a specific question about it, I'd be happy to try and help.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"input\": \"I couldn't understand it?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It doesn't remeber the previous messages and chat history.  We'll add this feature now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Chat History\n",
    "\n",
    "`1. History Aware Retriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002567269C4D0>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question \\n    which might reference context in the chat history, \\n    formulate a standalone question which can be understood \\n    without the chat history. Do NOT answer the question, \\n    just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000256772A0D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000256768DD110>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002567269C4D0>)), config={'run_name': 'chat_retriever_chain'})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "condense_q_system_template = (\n",
    "    \"\"\"Given a chat history and the latest user question \n",
    "    which might reference context in the chat history, \n",
    "    formulate a standalone question which can be understood \n",
    "    without the chat history. Do NOT answer the question, \n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    ")\n",
    "\n",
    "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_q_system_template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, condense_q_prompt\n",
    ")\n",
    "\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. Q/A Chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks.\\n    Use the following pieces of retrieved context to answer\\n    the question. If you don't know the answer, say that you\\n    don't know. Keep the\\n    answer concise.\\n\\n\\n\\n    {context}\\n    \")), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000256772A0D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000256768DD110>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer\n",
    "    the question. If you don't know the answer, say that you\n",
    "    don't know. Keep the\n",
    "    answer concise.\\n\\n\n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "qa_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. Retrieval Chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002567269C4D0>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question \\n    which might reference context in the chat history, \\n    formulate a standalone question which can be understood \\n    without the chat history. Do NOT answer the question, \\n    just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000256772A0D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000256768DD110>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002567269C4D0>)), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks.\\n    Use the following pieces of retrieved context to answer\\n    the question. If you don't know the answer, say that you\\n    don't know. Keep the\\n    answer concise.\\n\\n\\n\\n    {context}\\n    \")), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000256772A0D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000256768DD110>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question No.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Streaming is a way to process and yield output in chunks, rather than all at once, allowing for a more responsive \n",
       "and interactive experience, especially when working with large language models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> that can take a long time to \n",
       "generate a complete response.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Streaming is a way to process and yield output in chunks, rather than all at once, allowing for a more responsive \n",
       "and interactive experience, especially when working with large language models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m that can take a long time to \n",
       "generate a complete response.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "question = \"What is streaming?\"\n",
    "\n",
    "response = rag_chain.invoke(\n",
    "    {\n",
    "        \"input\" : question,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content = question),\n",
    "        AIMessage(content = response[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question No.2 (Follow-up Question)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In simple terms, streaming is a way to break down a big task, like generating a long response from a language \n",
       "model, into smaller chunks, and send those chunks one by one, rather than all at once.\n",
       "\n",
       "Think of it like a chef cooking a meal. Instead of serving the whole meal at once, the chef serves each course one \n",
       "by one, so you can enjoy each part of the meal as it's ready. That's similar to how streaming works with language \n",
       "models.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In simple terms, streaming is a way to break down a big task, like generating a long response from a language \n",
       "model, into smaller chunks, and send those chunks one by one, rather than all at once.\n",
       "\n",
       "Think of it like a chef cooking a meal. Instead of serving the whole meal at once, the chef serves each course one \n",
       "by one, so you can enjoy each part of the meal as it's ready. That's similar to how streaming works with language \n",
       "models.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is streaming?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Streaming is a way to process and yield output in chunks, rather than all at once, allowing for a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more responsive and interactive experience, especially when working with large language models (LLMs) that can take</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a long time to generate a complete response.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Can you explain it more because I couldn't get what you said.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"In simple terms, streaming is a way to break down a big task, like generating a long response from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a language model, into smaller chunks, and send those chunks one by one, rather than all at once.\\n\\nThink of it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">like a chef cooking a meal. Instead of serving the whole meal at once, the chef serves each course one by one, so </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you can enjoy each part of the meal as it's ready. That's similar to how streaming works with language models.\"</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is streaming?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Streaming is a way to process and yield output in chunks, rather than all at once, allowing for a \u001b[0m\n",
       "\u001b[32mmore responsive and interactive experience, especially when working with large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that can take\u001b[0m\n",
       "\u001b[32ma long time to generate a complete response.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"Can\u001b[0m\u001b[32m you explain it more because I couldn't get what you said.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m\"In\u001b[0m\u001b[32m simple terms, streaming is a way to break down a big task, like generating a long response from\u001b[0m\n",
       "\u001b[32ma language model, into smaller chunks, and send those chunks one by one, rather than all at once.\\n\\nThink of it \u001b[0m\n",
       "\u001b[32mlike a chef cooking a meal. Instead of serving the whole meal at once, the chef serves each course one by one, so \u001b[0m\n",
       "\u001b[32myou can enjoy each part of the meal as it's ready. That's similar to how streaming works with language models.\"\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question2= \"Can you explain it more because I couldn't get what you said.\"\n",
    "\n",
    "response = rag_chain.invoke(\n",
    "    {\n",
    "        \"input\" : question2,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content = question2),\n",
    "        AIMessage(content = response[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "print(response[\"answer\"])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question No.3 (Follow-up Question)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In the context of LangChain, streaming is implemented through the `stream` and `astream` methods. Here's an \n",
       "example:\n",
       "\n",
       "```python\n",
       "chain = <span style=\"font-weight: bold\">(</span>model | <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">JsonOutputParser</span><span style=\"font-weight: bold\">()</span> | _extract_country_names<span style=\"font-weight: bold\">)</span>\n",
       "for chunk in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chain.stream</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"input\"</span><span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>chunk<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "In this example, the `stream` method is used to process the input in chunks, and the `chunk` variable is yielded \n",
       "for each chunk of output.\n",
       "\n",
       "Alternatively, you can use the `astream` method for asynchronous streaming:\n",
       "\n",
       "```python\n",
       "chain = <span style=\"font-weight: bold\">(</span>model | <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">JsonOutputParser</span><span style=\"font-weight: bold\">()</span> | _extract_country_names<span style=\"font-weight: bold\">)</span>\n",
       "for chunk in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chain.astream</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"input\"</span><span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>chunk<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "The `astream` method is similar to `stream`, but it's designed for asynchronous use and can be more efficient for \n",
       "large inputs.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In the context of LangChain, streaming is implemented through the `stream` and `astream` methods. Here's an \n",
       "example:\n",
       "\n",
       "```python\n",
       "chain = \u001b[1m(\u001b[0mmodel | \u001b[1;35mJsonOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m | _extract_country_names\u001b[1m)\u001b[0m\n",
       "for chunk in \u001b[1;35mchain.stream\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"input\"\u001b[0m\u001b[1m)\u001b[0m:\n",
       "    \u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0mchunk\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "In this example, the `stream` method is used to process the input in chunks, and the `chunk` variable is yielded \n",
       "for each chunk of output.\n",
       "\n",
       "Alternatively, you can use the `astream` method for asynchronous streaming:\n",
       "\n",
       "```python\n",
       "chain = \u001b[1m(\u001b[0mmodel | \u001b[1;35mJsonOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m | _extract_country_names\u001b[1m)\u001b[0m\n",
       "for chunk in \u001b[1;35mchain.astream\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"input\"\u001b[0m\u001b[1m)\u001b[0m:\n",
       "    \u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0mchunk\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "The `astream` method is similar to `stream`, but it's designed for asynchronous use and can be more efficient for \n",
       "large inputs.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is streaming?'),\n",
       " AIMessage(content='Streaming is a way to process and yield output in chunks, rather than all at once, allowing for a more responsive and interactive experience, especially when working with large language models (LLMs) that can take a long time to generate a complete response.'),\n",
       " HumanMessage(content=\"Can you explain it more because I couldn't get what you said.\"),\n",
       " AIMessage(content=\"In simple terms, streaming is a way to break down a big task, like generating a long response from a language model, into smaller chunks, and send those chunks one by one, rather than all at once.\\n\\nThink of it like a chef cooking a meal. Instead of serving the whole meal at once, the chef serves each course one by one, so you can enjoy each part of the meal as it's ready. That's similar to how streaming works with language models.\"),\n",
       " HumanMessage(content='Ok. Can you explain it now through code?'),\n",
       " AIMessage(content='In the context of LangChain, streaming is implemented through the `stream` and `astream` methods. Here\\'s an example:\\n\\n```python\\nchain = (model | JsonOutputParser() | _extract_country_names)\\nfor chunk in chain.stream(\"input\"):\\n    print(chunk)\\n```\\n\\nIn this example, the `stream` method is used to process the input in chunks, and the `chunk` variable is yielded for each chunk of output.\\n\\nAlternatively, you can use the `astream` method for asynchronous streaming:\\n\\n```python\\nchain = (model | JsonOutputParser() | _extract_country_names)\\nfor chunk in chain.astream(\"input\"):\\n    print(chunk)\\n```\\n\\nThe `astream` method is similar to `stream`, but it\\'s designed for asynchronous use and can be more efficient for large inputs.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question3= \"Ok. Can you explain it now through code?\"\n",
    "\n",
    "response = rag_chain.invoke(\n",
    "    {\n",
    "        \"input\" : question3,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content = question3),\n",
    "        AIMessage(content = response[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "print(response[\"answer\"])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinig this RAG Chain with ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First Session_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain streaming refers to the ability to stream content in chunks from LangChain primitives, such as LLMs, parsers, prompts, retrievers, and agents, to end-users in a responsive manner. This is achieved through the LangChain Runnable Interface, which provides two approaches to streaming: sync stream and async astream.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"ak216\"}}\n",
    "\n",
    "# first question of this session_id\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is langchain streaming?\"},\n",
    "    config=config\n",
    ")['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To use LangChain streaming, primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface, which provides two approaches: sync stream and async astream. These methods allow streaming of final output in chunks. For async streaming, use async throughout the code, propagate callbacks, and call .astream() on LLMs to stream tokens.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How to use it?\"},\n",
    "    config=config\n",
    ")['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Second Session_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You\\'re referring to the code for propagating callbacks correctly. Here it is:\\n\\n```python\\nfrom langchain.chains import Chain\\nfrom langchain.chains.lambdas import Lambda\\nfrom langchain.tools import Tool\\n\\ndef reverse_and_double(input_str: str):\\n    return double_tool.invoke(reverse_word.invoke(input_str))\\n\\nreverse_word = Lambda(lambda x: x[::-1])\\ndouble_tool = Tool(lambda x: x*2)\\n\\nreverse_and_double = Chain([reverse_word, double_tool])\\n\\nawait reverse_and_double.invoke(\"1234\")\\n\\nasync for event in reverse_and_double.stream_events(\"1234\", version=\"v1\"):\\n    print(event)\\n```\\n\\nThis code creates a chain of two tools: `reverse_word` and `double_tool`. The `reverse_word` tool reverses the input string, and the `double_tool` tool doubles the input string. The `Chain` class is used to combine these tools into a single chain, and the `stream_events` method is used to generate stream events for the chain.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"aasher123\"}}\n",
    "\n",
    "# first question of this new session_id\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Can you give me its code?\"},  # It won't be able to answer as this is new id\n",
    "    config=config\n",
    ")['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
