{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine with Tools & Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In-built Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the inbuilt tool of wikipedia\n",
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=300)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_arx = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=300)\n",
    "arx_tool = ArxivQueryRun(api_wrapper=api_wrapper_arx)\n",
    "arx_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki_tool, arx_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://ts.llamaindex.ai/modules/llamacloud', 'title': 'LlamaCloud | LlamaIndex.TS', 'description': 'LlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.', 'language': 'en'}, page_content='LlamaCloud | LlamaIndex.TS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLlamaIndex.TSDocsEnglishEnglishBlogGitHubSearchWhat is LlamaIndex?Getting StartedGuidesExamplesRecipesModulesDocument / NodesPromptsLoadersVector StoresIngestion PipelineQuery EnginesAgentsEmbeddingsEvaluatingLLMsNode PostprocessorsChatEngineIndexNodeParserRetrieverResponseSynthesizerStorageLlamaCloudObservabilityllamaindexModulesLlamaCloudOn this pageLlamaCloud\\nLlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.\\nCurrently, LlamaCloud supports\\n\\nManaged Ingestion API, handling parsing and document management\\nManaged Retrieval API, configuring optimal retrieval for your RAG system'),\n",
       " Document(metadata={'source': 'https://ts.llamaindex.ai/modules/llamacloud', 'title': 'LlamaCloud | LlamaIndex.TS', 'description': 'LlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.', 'language': 'en'}, page_content=\"Access\\u200b\\nWe are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you’re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come talk to us.\\nIf you have access to LlamaCloud, you can visit LlamaCloud to sign in and get an API key.\\nCreate a Managed Index\\u200b\\nCurrently, you can't create a managed index on LlamaCloud using LlamaIndexTS, but you can use an existing managed index for retrieval that was created by the Python version of LlamaIndex. See the LlamaCloudIndex documentation for more information on how to create a managed index.\\nUse a Managed Index\\u200b\\nHere's an example of how to use a managed index together with a chat engine:\"),\n",
       " Document(metadata={'source': 'https://ts.llamaindex.ai/modules/llamacloud', 'title': 'LlamaCloud | LlamaIndex.TS', 'description': 'LlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.', 'language': 'en'}, page_content='Use a Managed Index\\u200b\\nHere\\'s an example of how to use a managed index together with a chat engine:\\nimport { stdin as input, stdout as output } from \"node:process\";import readline from \"node:readline/promises\";import { ContextChatEngine, LlamaCloudIndex } from \"llamaindex\";async function main() {  const index = new LlamaCloudIndex({    name: \"test\",    projectName: \"Default\",    baseUrl: process.env.LLAMA_CLOUD_BASE_URL,    apiKey: process.env.LLAMA_CLOUD_API_KEY,  });  const retriever = index.asRetriever({    similarityTopK: 5,  });  const chatEngine = new ContextChatEngine({ retriever });  const rl = readline.createInterface({ input, output });  while (true) {    const query = await rl.question(\"User: \");    const stream = await chatEngine.chat({ message: query, stream: true });    for await (const chunk of stream) {      process.stdout.write(chunk.response);    }    process.stdout.write(\"\\\\n\");  }}main().catch(console.error);\\nAPI Reference\\u200b'),\n",
       " Document(metadata={'source': 'https://ts.llamaindex.ai/modules/llamacloud', 'title': 'LlamaCloud | LlamaIndex.TS', 'description': 'LlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.', 'language': 'en'}, page_content='LlamaCloudIndex\\nLlamaCloudRetriever\\nPreviousStorageNextObservabilityAccessCreate a Managed IndexUse a Managed IndexAPI ReferenceDocsAPICommunityDiscordTwitterMoreGitHubCopyright © 2024 LlamaIndex. Built with Docusaurus.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(web_path=\"https://ts.llamaindex.ai/modules/llamacloud\")\n",
    "documents = loader.load()\n",
    "doc_chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(documents)\n",
    "doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024F609EFC10>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(doc_chunks, HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llamacloud-search'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turning the retriever into tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever=retriever, name=\"Llamacloud-search\",\n",
    "                                       description=\"Search any information about llamacloud\")\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'd:\\\\Projects and Learning\\\\Langchain\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=300)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=300)),\n",
       " Tool(name='Llamacloud-search', description='Search any information about llamacloud', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000024F413D3060>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024F609EFC10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000024F413D2E80>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024F609EFC10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combinig all tools together\n",
    "tools = [wiki_tool, arx_tool, retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running all tools with Agents and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt template\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024F7FE8B310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024F7FE54350>, model_name='llama-3.1-70b-versatile', groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'Llamacloud-search', 'description': 'Search any information about llamacloud', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024F7FE8B310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024F7FE54350>, model_name='llama-3.1-70b-versatile', groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'Llamacloud-search', 'description': 'Search any information about llamacloud', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'd:\\\\Projects and Learning\\\\Langchain\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=300)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=300)), Tool(name='Llamacloud-search', description='Search any information about llamacloud', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000024F413D3060>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024F609EFC10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000024F413D2E80>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024F609EFC10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Llamacloud-search` with `{'query': 'Llamacloud'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mLlamaCloudIndex\n",
      "LlamaCloudRetriever\n",
      "PreviousStorageNextObservabilityAccessCreate a Managed IndexUse a Managed IndexAPI ReferenceDocsAPICommunityDiscordTwitterMoreGitHubCopyright © 2024 LlamaIndex. Built with Docusaurus.\n",
      "\n",
      "LlamaCloud | LlamaIndex.TS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentLlamaIndex.TSDocsEnglishEnglishBlogGitHubSearchWhat is LlamaIndex?Getting StartedGuidesExamplesRecipesModulesDocument / NodesPromptsLoadersVector StoresIngestion PipelineQuery EnginesAgentsEmbeddingsEvaluatingLLMsNode PostprocessorsChatEngineIndexNodeParserRetrieverResponseSynthesizerStorageLlamaCloudObservabilityllamaindexModulesLlamaCloudOn this pageLlamaCloud\n",
      "LlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.\n",
      "Currently, LlamaCloud supports\n",
      "\n",
      "Managed Ingestion API, handling parsing and document management\n",
      "Managed Retrieval API, configuring optimal retrieval for your RAG system\n",
      "\n",
      "Access​\n",
      "We are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you’re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come talk to us.\n",
      "If you have access to LlamaCloud, you can visit LlamaCloud to sign in and get an API key.\n",
      "Create a Managed Index​\n",
      "Currently, you can't create a managed index on LlamaCloud using LlamaIndexTS, but you can use an existing managed index for retrieval that was created by the Python version of LlamaIndex. See the LlamaCloudIndex documentation for more information on how to create a managed index.\n",
      "Use a Managed Index​\n",
      "Here's an example of how to use a managed index together with a chat engine:\n",
      "\n",
      "Use a Managed Index​\n",
      "Here's an example of how to use a managed index together with a chat engine:\n",
      "import { stdin as input, stdout as output } from \"node:process\";import readline from \"node:readline/promises\";import { ContextChatEngine, LlamaCloudIndex } from \"llamaindex\";async function main() {  const index = new LlamaCloudIndex({    name: \"test\",    projectName: \"Default\",    baseUrl: process.env.LLAMA_CLOUD_BASE_URL,    apiKey: process.env.LLAMA_CLOUD_API_KEY,  });  const retriever = index.asRetriever({    similarityTopK: 5,  });  const chatEngine = new ContextChatEngine({ retriever });  const rl = readline.createInterface({ input, output });  while (true) {    const query = await rl.question(\"User: \");    const stream = await chatEngine.chat({ message: query, stream: true });    for await (const chunk of stream) {      process.stdout.write(chunk.response);    }    process.stdout.write(\"\\n\");  }}main().catch(console.error);\n",
      "API Reference​\u001b[0m\u001b[32;1m\u001b[1;3mLlamaCloud is a managed parsing, ingestion, and retrieval service designed to bring production-grade context-augmentation to LLM and RAG applications. It currently supports managed ingestion and retrieval APIs, and is available as a private beta to a limited set of enterprise partners.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the Llamacloud?',\n",
       " 'output': 'LlamaCloud is a managed parsing, ingestion, and retrieval service designed to bring production-grade context-augmentation to LLM and RAG applications. It currently supports managed ingestion and retrieval APIs, and is available as a private beta to a limited set of enterprise partners.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoking the agent executor\n",
    "agent_executor.invoke({\"input\": \"What is the Llamacloud?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Generative AI'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Generative artificial intelligence\n",
      "Summary: Generative artificial intelligence (generative AI, GenAI, or GAI) is artificial intelligence capable of generating text, images, videos, or other data using generative models, often in response to prompts. Generative AI models learn the patterns and \u001b[0m\u001b[32;1m\u001b[1;3mBased on the tool output, here is the final answer:\n",
      "\n",
      "Generative artificial intelligence (generative AI, GenAI, or GAI) is artificial intelligence capable of generating text, images, videos, or other data using generative models, often in response to prompts. Generative AI models learn the patterns and relationships in large datasets and can generate new, synthetic data that resembles the original data.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Generative AI?',\n",
       " 'output': 'Based on the tool output, here is the final answer:\\n\\nGenerative artificial intelligence (generative AI, GenAI, or GAI) is artificial intelligence capable of generating text, images, videos, or other data using generative models, often in response to prompts. Generative AI models learn the patterns and relationships in large datasets and can generate new, synthetic data that resembles the original data.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is Generative AI?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks \u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Attention Is All You Need\" by Ashish Vaswani et al. presents a novel approach to sequence transduction models, which are commonly used in natural language processing tasks. The authors propose a new architecture that relies solely on self-attention mechanisms, eliminating the need for recurrent or convolutional neural networks. This approach is claimed to be more efficient and effective than traditional models.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Tell me what's this paper 1706.03762 all about?\",\n",
       " 'output': 'The paper \"Attention Is All You Need\" by Ashish Vaswani et al. presents a novel approach to sequence transduction models, which are commonly used in natural language processing tasks. The authors propose a new architecture that relies solely on self-attention mechanisms, eliminating the need for recurrent or convolutional neural networks. This approach is claimed to be more efficient and effective than traditional models.'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Tell me what's this paper 1706.03762 all about?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
