{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002758AF23B10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002758AF106D0>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Aasher! Nice to meet you! As a student of GenAI (Generative Artificial Intelligence), you must be exploring the fascinating world of AI and machine learning. What specific aspects of GenAI are you interested in or currently studying?', response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 20, 'total_tokens': 69, 'completion_time': 0.065333333, 'prompt_time': 0.005499023, 'queue_time': None, 'total_time': 0.070832356}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-5ddf2b7b-1d59-4852-bd24-9db1965de268-0', usage_metadata={'input_tokens': 20, 'output_tokens': 49, 'total_tokens': 69})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke(\n",
    "    [HumanMessage(content=\"I'm Aasher, a student of GenAI\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I know that you're a student of GenAI, which means you're learning about Generative Artificial Intelligence. That's about the extent of my knowledge about you so far!\\n\\nAs we chat, I'll learn more about your interests, goals, and questions related to GenAI. I'll do my best to provide helpful information and insights to support your learning journey.\\n\\nSo, what's on your mind? Would you like to discuss a specific topic, ask a question, or explore a particular aspect of GenAI?\", response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 154, 'total_tokens': 258, 'completion_time': 0.138666667, 'prompt_time': 0.034493862, 'queue_time': None, 'total_time': 0.173160529}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-517a855a-5dd7-460e-a9e4-2020bacea2b6-0', usage_metadata={'input_tokens': 154, 'output_tokens': 104, 'total_tokens': 258})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "# trying to check if the model remebers previous messages by manually passing all the messages\n",
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"I'm Aasher, a student of GenAI\"),\n",
    "        AIMessage(content=\"Hello Aasher!\\n\\nWelcome to the GenAI student community! I'm excited to meet you and explore the fascinating world of Generative AI with you.\\n\\nWhat brings you to this field of study? Are you interested in exploring the capabilities of AI in generating human-like text, images, or music? Or perhaps you're curious about the applications of GenAI in areas such as language translation, creative writing, or even art?\\n\\nFeel free to share your goals, interests, or any specific aspects of GenAI that you'd like to discuss. I'm here to help and learn alongside you!\"),\n",
    "        HumanMessage(content=\"What do you know about me?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use message history instead of passing all previous messages manually.\n",
    "## Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # for chains\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]\n",
    "# runnable with message history\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"user1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"I'm Aasher, a student of GenAI\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Nice to meet you, Aasher! As a student of GenAI, you must be interested in the exciting field of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Artificial Intelligence and its applications. What specific areas of GenAI are you exploring or learning about? </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Natural Language Processing, Computer Vision, Reinforcement Learning, or something else?'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.078666667</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005665163</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'queue_time'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08433183</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama-3.1-8b-instant'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_9cb648b966'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-8e08a14c-0e16-4bbd-b555-4e00385fa15c-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Nice to meet you, Aasher! As a student of GenAI, you must be interested in the exciting field of \u001b[0m\n",
       "\u001b[32mArtificial Intelligence and its applications. What specific areas of GenAI are you exploring or learning about? \u001b[0m\n",
       "\u001b[32mNatural Language Processing, Computer Vision, Reinforcement Learning, or something else?'\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m59\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m20\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m79\u001b[0m,\n",
       "            \u001b[32m'completion_time'\u001b[0m: \u001b[1;36m0.078666667\u001b[0m,\n",
       "            \u001b[32m'prompt_time'\u001b[0m: \u001b[1;36m0.005665163\u001b[0m,\n",
       "            \u001b[32m'queue_time'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'total_time'\u001b[0m: \u001b[1;36m0.08433183\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'llama-3.1-8b-instant'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_9cb648b966'\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-8e08a14c-0e16-4bbd-b555-4e00385fa15c-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m20\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m59\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m79\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Nice to meet you, Aasher! As a student of GenAI, you must be interested in the exciting field of Artificial \n",
       "Intelligence and its applications. What specific areas of GenAI are you exploring or learning about? Natural \n",
       "Language Processing, Computer Vision, Reinforcement Learning, or something else?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Nice to meet you, Aasher! As a student of GenAI, you must be interested in the exciting field of Artificial \n",
       "Intelligence and its applications. What specific areas of GenAI are you exploring or learning about? Natural \n",
       "Language Processing, Computer Vision, Reinforcement Learning, or something else?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me earlier that you're Aasher, a student of GenAI!\", response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 92, 'total_tokens': 109, 'completion_time': 0.022666667, 'prompt_time': 0.028486911, 'queue_time': None, 'total_time': 0.051153578000000005}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-59fe2ed2-c573-4d99-aabe-82594f520a88-0', usage_metadata={'input_tokens': 92, 'output_tokens': 17, 'total_tokens': 109})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now invoking the same runnable and checking if it remembers our previous messages.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Who am I?\")],\n",
    "    config=config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Unfortunately, I don't have any information about you, so I'll have to take a wild guess. Here are a few possibilities:\\n\\n1. You're a human being, a unique individual with your own thoughts, feelings, and experiences.\\n2. You're a conversational AI (like me!), designed to simulate human-like conversations.\\n3. You're a fictional character, created by someone else's imagination.\\n4. You're a person from a specific culture, country, or time period, with a rich history and identity.\\n\\nTo help me narrow down the possibilities, can you give me a hint or provide some context about who you are?\", response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 14, 'total_tokens': 144, 'completion_time': 0.173333333, 'prompt_time': 0.00413512, 'queue_time': None, 'total_time': 0.177468453}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-805edcd9-b00b-42fd-b303-18dd922336f5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 130, 'total_tokens': 144})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing session_id\n",
    "config1 = {\"configurable\": {\"session_id\": \"user2\"}}\n",
    "# now it won't have previous messages history\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Who am I?\")],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Ahmad!\\n\\nAs a graphic designer, I'm sure you're creative, detail-oriented, and skilled in visual communication. You might be working on a variety of projects, from logos and branding to print materials and digital media.\\n\\nCan you tell me a bit more about yourself, Ahmad? For example:\\n\\n* What kind of design work do you enjoy most? (e.g., UI/UX, visual identity, illustration, etc.)\\n* What inspires your creative process?\\n* Do you have a favorite design tool or software?\\n* Are you working on any exciting projects currently?\\n\\nFeel free to share as much or as little as you'd like, and I'll do my best to chat with you about design, creativity, and more!\", response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 160, 'total_tokens': 311, 'completion_time': 0.201333333, 'prompt_time': 0.035991677, 'queue_time': None, 'total_time': 0.23732501}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-9b687fe3-a391-4a4c-b5f9-f9c5f5b7985f-0', usage_metadata={'input_tokens': 160, 'output_tokens': 151, 'total_tokens': 311})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"I'm Ahmad, a graphic designer\")],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"As we've just started our conversation, I don't know much about you beyond what you've told me:\\n\\n1. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Your name is Ahmad.\\n2. You're a graphic designer.\\n\\nHowever, I can try to make some educated guesses or ask </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions to learn more about you. Keep in mind that I'm a large language model, I don't have any personal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge or records about you, and this conversation is a fresh start.\\n\\nHere are a few questions to get us </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">started:\\n\\n* Where are you from (country, city, or region)?\\n* What kind of design projects have you worked on in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the past (e.g., logos, brochures, websites, etc.)?\\n* Do you have any favorite designers, artists, or creatives who</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inspire you?\\n* Are you working independently or as part of a design team?\\n\\nFeel free to answer any or all of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these questions, and I'll do my best to learn more about you and start a conversation!\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">196</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">327</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">523</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.261333333</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.072649006</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'queue_time'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.333982339</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama-3.1-8b-instant'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_9cb648b966'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-98aee62b-e06a-4bdb-80d8-5b525c6203e7-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">327</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">196</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">523</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"As\u001b[0m\u001b[32m we've just started our conversation, I don't know much about you beyond what you've told me:\\n\\n1. \u001b[0m\n",
       "\u001b[32mYour name is Ahmad.\\n2. You're a graphic designer.\\n\\nHowever, I can try to make some educated guesses or ask \u001b[0m\n",
       "\u001b[32mquestions to learn more about you. Keep in mind that I'm a large language model, I don't have any personal \u001b[0m\n",
       "\u001b[32mknowledge or records about you, and this conversation is a fresh start.\\n\\nHere are a few questions to get us \u001b[0m\n",
       "\u001b[32mstarted:\\n\\n* Where are you from \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcountry, city, or region\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?\\n* What kind of design projects have you worked on in \u001b[0m\n",
       "\u001b[32mthe past \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., logos, brochures, websites, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?\\n* Do you have any favorite designers, artists, or creatives who\u001b[0m\n",
       "\u001b[32minspire you?\\n* Are you working independently or as part of a design team?\\n\\nFeel free to answer any or all of \u001b[0m\n",
       "\u001b[32mthese questions, and I'll do my best to learn more about you and start a conversation!\"\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m196\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m327\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m523\u001b[0m,\n",
       "            \u001b[32m'completion_time'\u001b[0m: \u001b[1;36m0.261333333\u001b[0m,\n",
       "            \u001b[32m'prompt_time'\u001b[0m: \u001b[1;36m0.072649006\u001b[0m,\n",
       "            \u001b[32m'queue_time'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'total_time'\u001b[0m: \u001b[1;36m0.333982339\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'llama-3.1-8b-instant'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_9cb648b966'\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-98aee62b-e06a-4bdb-80d8-5b525c6203e7-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m327\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m196\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m523\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What do you know about me?\")],\n",
    "    config=config1\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"I'm Aasher, a student of GenAI\"), AIMessage(content='Nice to meet you, Aasher! As a student of GenAI, you must be interested in the exciting field of Artificial Intelligence and its applications. What specific areas of GenAI are you exploring or learning about? Natural Language Processing, Computer Vision, Reinforcement Learning, or something else?', response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 20, 'total_tokens': 79, 'completion_time': 0.078666667, 'prompt_time': 0.005665163, 'queue_time': None, 'total_time': 0.08433183}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-8e08a14c-0e16-4bbd-b555-4e00385fa15c-0', usage_metadata={'input_tokens': 20, 'output_tokens': 59, 'total_tokens': 79}), HumanMessage(content='Who am I?'), AIMessage(content=\"You told me earlier that you're Aasher, a student of GenAI!\", response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 92, 'total_tokens': 109, 'completion_time': 0.022666667, 'prompt_time': 0.028486911, 'queue_time': None, 'total_time': 0.051153578000000005}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-59fe2ed2-c573-4d99-aabe-82594f520a88-0', usage_metadata={'input_tokens': 92, 'output_tokens': 17, 'total_tokens': 109})]),\n",
       " 'user2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Who am I?'), AIMessage(content=\"Unfortunately, I don't have any information about you, so I'll have to take a wild guess. Here are a few possibilities:\\n\\n1. You're a human being, a unique individual with your own thoughts, feelings, and experiences.\\n2. You're a conversational AI (like me!), designed to simulate human-like conversations.\\n3. You're a fictional character, created by someone else's imagination.\\n4. You're a person from a specific culture, country, or time period, with a rich history and identity.\\n\\nTo help me narrow down the possibilities, can you give me a hint or provide some context about who you are?\", response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 14, 'total_tokens': 144, 'completion_time': 0.173333333, 'prompt_time': 0.00413512, 'queue_time': None, 'total_time': 0.177468453}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-805edcd9-b00b-42fd-b303-18dd922336f5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 130, 'total_tokens': 144}), HumanMessage(content=\"I'm Ahmad, a graphic designer\"), AIMessage(content=\"Nice to meet you, Ahmad!\\n\\nAs a graphic designer, I'm sure you're creative, detail-oriented, and skilled in visual communication. You might be working on a variety of projects, from logos and branding to print materials and digital media.\\n\\nCan you tell me a bit more about yourself, Ahmad? For example:\\n\\n* What kind of design work do you enjoy most? (e.g., UI/UX, visual identity, illustration, etc.)\\n* What inspires your creative process?\\n* Do you have a favorite design tool or software?\\n* Are you working on any exciting projects currently?\\n\\nFeel free to share as much or as little as you'd like, and I'll do my best to chat with you about design, creativity, and more!\", response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 160, 'total_tokens': 311, 'completion_time': 0.201333333, 'prompt_time': 0.035991677, 'queue_time': None, 'total_time': 0.23732501}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-9b687fe3-a391-4a4c-b5f9-f9c5f5b7985f-0', usage_metadata={'input_tokens': 160, 'output_tokens': 151, 'total_tokens': 311}), HumanMessage(content='What do you know about me?'), AIMessage(content=\"As we've just started our conversation, I don't know much about you beyond what you've told me:\\n\\n1. Your name is Ahmad.\\n2. You're a graphic designer.\\n\\nHowever, I can try to make some educated guesses or ask questions to learn more about you. Keep in mind that I'm a large language model, I don't have any personal knowledge or records about you, and this conversation is a fresh start.\\n\\nHere are a few questions to get us started:\\n\\n* Where are you from (country, city, or region)?\\n* What kind of design projects have you worked on in the past (e.g., logos, brochures, websites, etc.)?\\n* Do you have any favorite designers, artists, or creatives who inspire you?\\n* Are you working independently or as part of a design team?\\n\\nFeel free to answer any or all of these questions, and I'll do my best to learn more about you and start a conversation!\", response_metadata={'token_usage': {'completion_tokens': 196, 'prompt_tokens': 327, 'total_tokens': 523, 'completion_time': 0.261333333, 'prompt_time': 0.072649006, 'queue_time': None, 'total_time': 0.333982339}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-98aee62b-e06a-4bdb-80d8-5b525c6203e7-0', usage_metadata={'input_tokens': 327, 'output_tokens': 196, 'total_tokens': 523})])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store    #it contains all session_ids and their message histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant. Answer all questions to the best of your knowledge.')), MessagesPlaceholder(variable_name='messages')])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt  = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all questions to the best of your knowledge.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Aasher! Nice to meet you! Is there something I can help you with or would you like to chat?', response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 37, 'total_tokens': 62, 'completion_time': 0.033333333, 'prompt_time': 0.009484017, 'queue_time': None, 'total_time': 0.042817350000000004}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-96127635-af3a-4989-be20-c58d2c41428d-0', usage_metadata={'input_tokens': 37, 'output_tokens': 25, 'total_tokens': 62})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"messages\": [HumanMessage(content=\"Hi, Aasher here!\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"user3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"I'm Aasher. How are you?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Nice to meet you, Aasher! I'm doing well, thank you for asking. I'm a helpful assistant, and I'm here </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to assist you with any questions or tasks you may have. How's your day going so far?\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">140</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.065333333</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.021124908</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'queue_time'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08645824099999999</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama-3.1-8b-instant'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_f66ccb39ec'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-c812f8f3-25c8-4fae-ba25-4a4d2c8c19a4-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">140</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"Nice\u001b[0m\u001b[32m to meet you, Aasher! I'm doing well, thank you for asking. I'm a helpful assistant, and I'm here \u001b[0m\n",
       "\u001b[32mto assist you with any questions or tasks you may have. How's your day going so far?\"\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m49\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m91\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m140\u001b[0m,\n",
       "            \u001b[32m'completion_time'\u001b[0m: \u001b[1;36m0.065333333\u001b[0m,\n",
       "            \u001b[32m'prompt_time'\u001b[0m: \u001b[1;36m0.021124908\u001b[0m,\n",
       "            \u001b[32m'queue_time'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'total_time'\u001b[0m: \u001b[1;36m0.08645824099999999\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'llama-3.1-8b-instant'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_f66ccb39ec'\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-c812f8f3-25c8-4fae-ba25-4a4d2c8c19a4-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m91\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m49\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m140\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Aasher.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 154, 'total_tokens': 161, 'completion_time': 0.009333333, 'prompt_time': 0.034805366, 'queue_time': None, 'total_time': 0.044138698999999997}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-bb2661b3-6221-414d-9d53-7bdd02705276-0', usage_metadata={'input_tokens': 154, 'output_tokens': 7, 'total_tokens': 161})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make it more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], template='You are a helpful assistant. Answer all questions to the best of your  in {language}.')), MessagesPlaceholder(variable_name='messages')])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002758AF23B10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002758AF106D0>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all questions to the best of your  in {language}.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\": [HumanMessage(content=\"Hey there! MY name is Aasher. How are you?\")],\n",
    "    \"language\": \"French\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Bonjour Aasher ! Je vais bien, merci ! C'est un plaisir de faire votre connaissance. Comment allez-vous aujourd'hui\n",
       "? <span style=\"font-weight: bold\">(</span>Hello Aasher! I'm doing well, thank you! Nice to meet you. How are you today?<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Bonjour Aasher ! Je vais bien, merci ! C'est un plaisir de faire votre connaissance. Comment allez-vous aujourd'hui\n",
       "? \u001b[1m(\u001b[0mHello Aasher! I'm doing well, thank you! Nice to meet you. How are you today?\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapping this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, # adding a chain instead of llm\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    "    )\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"user4\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hey there! My name is Aasher. How are you?\")],\n",
    "        \"language\": \"Arabic\"\n",
    "    },\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">مرحباً أشهره! <span style=\"font-weight: bold\">(</span>Marhaba Ashsher!<span style=\"font-weight: bold\">)</span> Nice to meet you! أنا بخير شكراً <span style=\"font-weight: bold\">(</span>Ana kheir shukraan<span style=\"font-weight: bold\">)</span> Thank you, I'm doing great! \n",
       "How can I assist you today, أشر؟ <span style=\"font-weight: bold\">(</span>Asher<span style=\"font-weight: bold\">)</span>?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "مرحباً أشهره! \u001b[1m(\u001b[0mMarhaba Ashsher!\u001b[1m)\u001b[0m Nice to meet you! أنا بخير شكراً \u001b[1m(\u001b[0mAna kheir shukraan\u001b[1m)\u001b[0m Thank you, I'm doing great! \n",
       "How can I assist you today, أشر؟ \u001b[1m(\u001b[0mAsher\u001b[1m)\u001b[0m?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check if it remebers the previous messages\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What's my name?\")],\n",
    "        \"language\": \"Urdu\"\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">آشر <span style=\"font-weight: bold\">(</span>Asher<span style=\"font-weight: bold\">)</span> تھا! آپ کا نام آشر ہے!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "آشر \u001b[1m(\u001b[0mAsher\u001b[1m)\u001b[0m تھا! آپ کا نام آشر ہے!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing the Chat History\n",
    "We'll put a limit on messages given to the model so that it doesn't cross its context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='I like apples'),\n",
       " AIMessage(content='Good to know'),\n",
       " HumanMessage(content='What is 5-3?'),\n",
       " AIMessage(content='2'),\n",
       " HumanMessage(content='What is 5*3?'),\n",
       " AIMessage(content='15'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content=\"You're welcome\")]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import  SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",  ## keeping most recent messages\n",
    "    token_counter=llm,\n",
    "    include_system=True,  # system message will always be included\n",
    "    allow_partial=False,  # to avoid half messages\n",
    "    start_on=\"human\"  # start from Human message\n",
    "\n",
    ")\n",
    "\n",
    "# dummy messages list\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi, I am Khalid?\"),\n",
    "    AIMessage(content=\"Hey there, how can I help you today?\"),\n",
    "    HumanMessage(content=\"I like apples\"),\n",
    "    AIMessage(content=\"Good to know\"),\n",
    "    HumanMessage(content=\"What is 5-3?\"),\n",
    "    AIMessage(content=\"2\"),\n",
    "    HumanMessage(content=\"What is 5*3?\"),\n",
    "    AIMessage(content=\"15\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"You're welcome\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See, the two messages have been trimmed!**\\\n",
    "If we increase max_tokens, we can get all messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  messages: RunnableLambda(itemgetter('messages'))\n",
       "            | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['language', 'messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], template='You are a helpful assistant. Answer all questions to the best of your  in {language}.')), MessagesPlaceholder(variable_name='messages')])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002758AF23B10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002758AF106D0>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter # to retrieve specific items from a sequence or mapping object\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What maths problems  did I ask?\")],\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You asked me to solve two basic math problems:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. A subtraction problem: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> - <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. A multiplication problem: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> * <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "Let me know if you'd like me to help with anything else!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You asked me to solve two basic math problems:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. A subtraction problem: \u001b[1;36m5\u001b[0m - \u001b[1;36m3\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m. A multiplication problem: \u001b[1;36m5\u001b[0m * \u001b[1;36m3\u001b[0m\n",
       "\n",
       "Let me know if you'd like me to help with anything else!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping this in Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"user6\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is the beginning of our conversation, and you haven't asked any math problems yet. I'm here to help, though! What math question do you have for me?\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What maths problems  did I ask?\")],\n",
    "        \"language\": \"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
